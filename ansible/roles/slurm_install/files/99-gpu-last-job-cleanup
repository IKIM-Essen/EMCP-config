#!/usr/bin/env sh
set -e

description="GPU cleanup for user $SLURM_JOB_USER on node $SLURMD_NODENAME"
echo "Starting $description."

if ! command -v /usr/bin/nvidia-smi > /dev/null 2>&1; then
	echo "Skipping as nvidia-smi is not installed on this node."
	exit 0
fi

if [ $SLURM_JOB_UID -eq 0 ]; then
	echo "Skipping for user root."
	exit 0
fi

nvidia_devices="$(/usr/bin/find /dev -regex '/dev/nvidia[0-9]+' -printf '%p ')"
remaining_jobs="$(/usr/bin/squeue -h -u "$SLURM_JOB_USER" -w "$SLURMD_NODENAME" --states=pending,running)"

if [ "$remaining_jobs" ]; then
	echo "Skipping as this is not the user's last job on the target nodes."
	exit 0
fi

# To address the issue of misbehaving GPU workloads which leave orphan processes
# with allocated GPU memory, after checking that the user doesn't have any
# running processes on the target nodes, kill any processes owned by the user
# that are still attached to the GPU.
#
# Since the sequence of checking -> killing is not atomic, no other instructions
# should be inserted between these two commands.
#
# The exit status of fuser is suppressed as the program returns non-zero when
# there are simply no processes to kill.
/usr/sbin/runuser -u "$SLURM_JOB_USER" -- /usr/bin/fuser -k $nvidia_devices || true

echo "Finished $description."
