# Inventory group names
slurm_controller_group_name: slurm_controller
slurm_worker_group_name: slurm_worker  # includes compute and user-facing nodes
slurm_compute_group_name: slurm_compute
slurm_dbd_group_name: slurm_database

# Upgrade mode: if disabled, the daemons are restarted as needed if a
# configuration change is detected; if enabled, the daemons are stopped
# unconditionally, then the the regular deployment takes place and finally they
# are started again.
# By keeping the daemons stopped for the duration of the deployment, the risk of
# leaving the cluster in an inconsistent state due to issues during an upgrade
# is reduced.
# In order to actually carry out an upgrade, the slurm_worker_debs_* variables
# must point to newer deb packages. Note that this playbook only supports
# installing slurmd packages; the other daemons, namely slurmctld and slurmdbd,
# can be upgraded by deploying new container images using the RKE2 playbook.
# The slurmctld and slurmdbd nodes should be upgraded before the slurmd nodes.
# See https://slurm.schedmd.com/quickstart_admin.html#upgrade
slurmd_upgrade: false

# URL of the archive containing the deb packages
slurm_worker_debs_url: "https://github.com/IKIM-Essen/EMCP-slurm/releases/download/{{ slurm_worker_debs_tag }}/{{ slurm_worker_debs_archive }}"

# Cluster name for the configuration file
slurm_cluster_name: slurmcluster

# Value of StateSaveLocation in the configuration file.
slurmctld_state_save_location: /var/spool/slurmctld

# Log directory
slurm_log_dir: /var/log/slurm

# The amount of memory on each node that is *not* intended to be usable by
# slurm jobs. The usable memory is obtained by subtracting slurm_reserved_mem_mb
# from the physical memory.
# Note that this policy is enforced only if memory is declared as a consumable
# resource in slurm.conf, in which case slurm prevents allocating more than the
# usable memory. If memory is not a consumable resource, the usable memory is
# simply the maximum that a job can request on a given node, but slurm will not
# enforce any limits.
# See https://slurm.schedmd.com/cons_res.html for details.
slurm_reserved_mem_mb: 4096

# User limits on worker nodes
slurm_ulimit_memlock_soft: "unlimited"
slurm_ulimit_memlock_hard: "unlimited"
slurm_ulimit_nofile_soft: "65536"  # 2^16
slurm_ulimit_nofile_hard: "131072"  # 2^17
slurm_ulimit_stack_soft: "67108864"  # 64 GiB
slurm_ulimit_stack_hard: "unlimited"

# Slurmdbd settings
slurmdbd_archive_events: "yes"
slurmdbd_archive_jobs: "yes"
slurmdbd_archive_reservations: "yes"
slurmdbd_archive_steps: "no"
slurmdbd_archive_suspend: "no"
slurmdbd_archive_transactions: "no"
slurmdbd_archive_usage: "no"

slurmdbd_purge_events_after: "1month"
slurmdbd_purge_jobs_after: "12month"
slurmdbd_purge_reservations_after: "1month"
slurmdbd_purge_steps_after: "1month"
slurmdbd_purge_suspend_after: "1month"
slurmdbd_purge_transactions_after: "12month"
slurmdbd_purge_usage_after: "24month"

slurmdbd_mariadb_url: ""
# The compute node partitions as a list of hashes.
# Each hash has the following keys:
#   name: the partition name as a string
#   hosts: either the special value "ALL" or a list of inventory hosts
#   default: the value of the "Default" parameter in slurm.conf (optional boolean, false if unspecified)
#   opts: custom options for the partition configuration in slurm.conf (optional string, empty if unspecified)
# Example:
#   - name: compute
#     hosts: "{{ groups['slurm_compute'] }}"
#     default: true
#     opts: AllowGroups=ipausers
#   - name: debug
#     hosts:
#       - t01.example.org
#       - t02.example.org
slurm_partitions:
  - name: batch
    hosts: ALL
    default: true
    opts: ""

# Destination directory of the local storage cleanup script on each node
slurm_cleanup_script_root: /opt/cleanup

# Parameters of the local storage cleanup script
# The cleanup script runs only if the ratio of free to total space on the
# specified paths is lower than slurm_local_storage_threshold.
slurm_local_storage_threshold: 0.2
slurm_local_storage_userdirs: []
slurm_local_storage_globaldirs: []
