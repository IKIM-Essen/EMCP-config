# Inventory group names
slurm_controller_group_name: slurm_controller
slurm_worker_group_name: slurm_worker

# The version to install.
slurm_version: "19.05.8"
slurm_version_with_release: "{{ slurm_version }}-1"

# URL of the archive containing the deb packages.
slurm_worker_debs_url: https://github.com/IKIM-Essen/EMCP-slurm/releases/download/v21.08.5/slurm-21.08.5-ubuntu22.04-debs.zip

# Packages to install on worker nodes as a list of paths relative to the
# directory where slurm_worker_debs_url is extracted.
slurm_worker_package_paths:
  - slurm-21.08.5-ubuntu22.04-debs/slurmd_21.08.5-2ubuntu1_amd64.deb
  - slurm-21.08.5-ubuntu22.04-debs/slurm-wlm-basic-plugins_21.08.5-2ubuntu1_amd64.deb
  - slurm-21.08.5-ubuntu22.04-debs/slurm-client_21.08.5-2ubuntu1_amd64.deb

# Optional: the SSSD domain that slurm should be able to enumerate.
# If undefined, defaults to ansible_facts['domain'].
# slurm_sssd_domain:

# URI of the package repository.
# The package repository can be built using the slurm_pkgbuild role.
# See `man sources.list` for the supported URI schemes.
slurm_repo_local_path: /projects/localrepo
slurm_repo_uri: "file:{{ slurm_repo_local_path }}"

# The munge secret which is hashed to create the munge key.
# It should be encrypted with Ansible Vault.
# It can be any value such as a random string generated with
# cat /dev/urandom | tr -dc '[:alnum:]' | head -c 128
# As a general indication, pick a length matching MUNGE_KEY_LEN_DFL_BYTES from
# from https://github.com/dun/munge/blob/master/src/libcommon/munge_defs.h
slurm_munge_secret: "{{ vault_slurm_munge_secret }}"

# Cluster name for the configuration file
slurm_cluster_name: slurmcluster

# Value of StateSaveLocation in the configuration file.
# It should match the variable of the same name in the k8s_slurmctld role.
slurmctld_state_save_location: /var/spool/slurmctld

# Log directory
slurm_log_dir: /var/log/slurm

# The amount of memory on each node that is *not* intended to be usable by
# slurm jobs. The usable memory is obtained by subtracting slurm_reserved_mem_mb
# from the physical memory.
# Note that this policy is enforced only if memory is declared as a consumable
# resource in slurm.conf, in which case slurm prevents allocating more than the
# usable memory. If memory is not a consumable resource, the usable memory is
# simply the maximum that a job can request on a given node, but slurm will not
# enforce any limits.
# See https://slurm.schedmd.com/cons_res.html for details.
slurm_reserved_mem_mb: 4096

# User limits on worker nodes
slurm_ulimit_memlock_soft: "unlimited"
slurm_ulimit_memlock_hard: "unlimited"
slurm_ulimit_nofile_soft: "65536"  # 2^16
slurm_ulimit_nofile_hard: "131072"  # 2^17
slurm_ulimit_stack_soft: "67108864"  # 64 GiB
slurm_ulimit_stack_hard: "unlimited"

# The compute node partitions as a list of hashes.
# Each hash has the following keys:
#   name: the partition name as a string
#   hosts: either the special value "ALL" or a list of inventory hosts
#   default: the value of the "Default" parameter in slurm.conf (optional boolean, false if unspecified)
#   opts: custom options for the partition configuration in slurm.conf (optional string, empty if unspecified)
# Example:
#   - name: compute
#     hosts: "{{ groups['slurm_compute'] }}"
#     default: true
#     opts: AllowGroups=ipausers
#   - name: debug
#     hosts:
#       - t01.example.org
#       - t02.example.org
slurm_partitions:
  - name: batch
    hosts: ALL
    default: true
    opts: ""
