# Inventory group names
slurm_controller_group_name: slurm_controller
slurm_worker_group_name: slurm_worker  # includes compute and user-facing nodes
slurm_compute_group_name: slurm_compute
slurm_dbd_group_name: slurm_database
rke2_primary_server_group_name: primary_rke_server

# Upgrade mode: if disabled, the daemons are restarted as needed if a
# configuration change is detected; if enabled, the daemons are stopped
# unconditionally, then the the regular deployment takes place and finally they
# are started again. By keeping the daemons stopped for the duration of the
# deployment, the risk of leaving the cluster in an inconsistent state due to
# an error is reduced.
slurm_upgrade: false

# URL of the archive containing the deb packages
slurm_worker_debs_url: "https://github.com/IKIM-Essen/EMCP-slurm/releases/download/{{ slurm_worker_debs_tag }}/{{ slurm_worker_debs_archive }}"

# Cluster name for the configuration file
slurm_cluster_name: slurmcluster

# Value of StateSaveLocation in the configuration file.
# It should match the variable of the same name in the k8s_slurmctld role.
slurmctld_state_save_location: /var/spool/slurmctld

# Log directory
slurm_log_dir: /var/log/slurm

# The amount of memory on each node that is *not* intended to be usable by
# slurm jobs. The usable memory is obtained by subtracting slurm_reserved_mem_mb
# from the physical memory.
# Note that this policy is enforced only if memory is declared as a consumable
# resource in slurm.conf, in which case slurm prevents allocating more than the
# usable memory. If memory is not a consumable resource, the usable memory is
# simply the maximum that a job can request on a given node, but slurm will not
# enforce any limits.
# See https://slurm.schedmd.com/cons_res.html for details.
slurm_reserved_mem_mb: 4096

# User limits on worker nodes
slurm_ulimit_memlock_soft: "unlimited"
slurm_ulimit_memlock_hard: "unlimited"
slurm_ulimit_nofile_soft: "65536"  # 2^16
slurm_ulimit_nofile_hard: "131072"  # 2^17
slurm_ulimit_stack_soft: "67108864"  # 64 GiB
slurm_ulimit_stack_hard: "unlimited"

# Slurmdbd settings
slurmdbd_archive_events: "yes"
slurmdbd_archive_jobs: "yes"
slurmdbd_archive_reservations: "yes"
slurmdbd_archive_steps: "no"
slurmdbd_archive_suspend: "no"
slurmdbd_archive_transactions: "no"
slurmdbd_archive_usage: "no"

slurmdbd_purge_events_after: "1month"
slurmdbd_purge_jobs_after: "12month"
slurmdbd_purge_reservations_after: "1month"
slurmdbd_purge_steps_after: "1month"
slurmdbd_purge_suspend_after: "1month"
slurmdbd_purge_transactions_after: "12month"
slurmdbd_purge_usage_after: "24month"

slurmdbd_mariadb_url: ""
# The compute node partitions as a list of hashes.
# Each hash has the following keys:
#   name: the partition name as a string
#   hosts: either the special value "ALL" or a list of inventory hosts
#   default: the value of the "Default" parameter in slurm.conf (optional boolean, false if unspecified)
#   opts: custom options for the partition configuration in slurm.conf (optional string, empty if unspecified)
# Example:
#   - name: compute
#     hosts: "{{ groups['slurm_compute'] }}"
#     default: true
#     opts: AllowGroups=ipausers
#   - name: debug
#     hosts:
#       - t01.example.org
#       - t02.example.org
slurm_partitions:
  - name: batch
    hosts: ALL
    default: true
    opts: ""

# Destination directory of the automated maintenance script on each node
slurm_node_maintenance_script_root: /opt/maintenance

# Parameters of the local storage cleanup logic in the maintenance script.
# Cleanup is carried out only if the ratio of free to total space on the
# specified paths is lower than slurm_local_storage_threshold.
slurm_local_storage_threshold: 0.2
slurm_local_storage_userdirs: []
slurm_local_storage_globaldirs: []
